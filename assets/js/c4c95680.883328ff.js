"use strict";(self.webpackChunkairsdk_dev=self.webpackChunkairsdk_dev||[]).push([[52623],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(96540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}},31046:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>l,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"development/rich-media-content/working-with-sound/capturing-sound-input","title":"Capturing sound input","description":"The Microphone class lets your application connect to a microphone or other","source":"@site/docs/development/rich-media-content/working-with-sound/capturing-sound-input.md","sourceDirName":"development/rich-media-content/working-with-sound","slug":"/development/rich-media-content/working-with-sound/capturing-sound-input","permalink":"/docs/development/rich-media-content/working-with-sound/capturing-sound-input","draft":false,"unlisted":false,"editUrl":"https://github.com/airsdk/airsdk.dev/edit/main/docs/development/rich-media-content/working-with-sound/capturing-sound-input.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"mainSidebar","previous":{"title":"Accessing raw sound data","permalink":"/docs/development/rich-media-content/working-with-sound/accessing-raw-sound-data"},"next":{"title":"Sound example: Podcast Player","permalink":"/docs/development/rich-media-content/working-with-sound/sound-example-podcast-player"}}');var i=t(74848),a=t(28453);const s={sidebar_position:12},r="Capturing sound input",c={},d=[{value:"Accessing a microphone",id:"accessing-a-microphone",level:2},{value:"Routing microphone audio to local speakers",id:"routing-microphone-audio-to-local-speakers",level:2},{value:"Altering microphone audio",id:"altering-microphone-audio",level:2},{value:"Detecting microphone activity",id:"detecting-microphone-activity",level:2},{value:"Sending audio to and from a media server",id:"sending-audio-to-and-from-a-media-server",level:2},{value:"Capturing microphone sound data",id:"capturing-microphone-sound-data",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capturing-sound-input",children:"Capturing sound input"})}),"\n",(0,i.jsx)(n.p,{children:"The Microphone class lets your application connect to a microphone or other\nsound input device on the user's system and broadcast the input audio to that\nsystem's speakers or send the audio data to a remote server, such as Flash Media\nServer. You can access the raw audio data from the microphone and record or\nprocess it; you can also send the audio directly to the system's speakers or\nsend compressed audio data to a remote server. You can use either Speex or\nNellymoser codec for data sent to a remote server. (The Speex codec is supported\nstarting with Flash Player 10 and Adobe AIR 1.5.)"}),"\n",(0,i.jsx)(n.h2,{id:"accessing-a-microphone",children:"Accessing a microphone"}),"\n",(0,i.jsxs)(n.p,{children:["The Microphone class does not have a constructor method. Instead, you use the\nstatic ",(0,i.jsx)(n.code,{children:"Microphone.getMicrophone()"})," method to obtain a new Microphone instance,\nas shown below:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"var mic:Microphone = Microphone.getMicrophone();\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Calling the ",(0,i.jsx)(n.code,{children:"Microphone.getMicrophone()"})," method without a parameter returns the\nfirst sound input device discovered on the user's system."]}),"\n",(0,i.jsxs)(n.p,{children:["A system can have more than one sound input device attached to it. Your\napplication can use the ",(0,i.jsx)(n.code,{children:"Microphone.names"})," property to get an array of the names\nof all available sound input devices. Then it can call the\n",(0,i.jsx)(n.code,{children:"Microphone.getMicrophone()"})," method with an ",(0,i.jsx)(n.code,{children:"index"})," parameter that matches the\nindex value of a device's name in the array."]}),"\n",(0,i.jsxs)(n.p,{children:["A system might not have a microphone or other sound input device attached to it.\nYou can use the ",(0,i.jsx)(n.code,{children:"Microphone.names"})," property or the ",(0,i.jsx)(n.code,{children:"Microphone.getMicrophone()"}),"\nmethod to check whether the user has a sound input device installed. If the user\ndoesn't have a sound input device installed, the ",(0,i.jsx)(n.code,{children:"names"})," array has a length of\nzero, and the ",(0,i.jsx)(n.code,{children:"getMicrophone()"})," method returns a value of ",(0,i.jsx)(n.code,{children:"null"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["When your application calls the ",(0,i.jsx)(n.code,{children:"Microphone.getMicrophone()"})," method, Flash\nPlayer displays the Flash Player Settings dialog box, which prompts the user to\neither allow or deny Flash Player access to the camera and microphone on the\nsystem. After the user clicks on either the Allow button or the Deny button in\nthis dialog, a StatusEvent is dispatched. The ",(0,i.jsx)(n.code,{children:"code"})," property of that\nStatusEvent instance indicates whether microphone access was allowed or denied,\nas shown in this example:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'import flash.media.Microphone;\n\nvar mic:Microphone = Microphone.getMicrophone();\nmic.addEventListener(StatusEvent.STATUS, this.onMicStatus);\n\nfunction onMicStatus(event:StatusEvent):void\n{\n\tif (event.code == "Microphone.Unmuted")\n\t{\n\t\ttrace("Microphone access was allowed.");\n\t}\n\telse if (event.code == "Microphone.Muted")\n\t{\n\t\ttrace("Microphone access was denied.");\n\t}\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"StatusEvent.code"}),' property will contain "Microphone.Unmuted" if access was\nallowed, or "Microphone.Muted" if access was denied.']}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"Microphone.muted"})," property is set to ",(0,i.jsx)(n.code,{children:"true"})," or ",(0,i.jsx)(n.code,{children:"false"})," when the user allows\nor denies microphone access, respectively. However, the ",(0,i.jsx)(n.code,{children:"muted"})," property is not\nset on the Microphone instance until the StatusEvent has been dispatched, so\nyour application should also wait for the ",(0,i.jsx)(n.code,{children:"StatusEvent.STATUS"})," event to be\ndispatched before checking the ",(0,i.jsx)(n.code,{children:"Microphone.muted"})," property."]}),"\n",(0,i.jsx)(n.p,{children:"In order for Flash Player to display the settings dialog, the application window\nmust be large enough to display it (at least 215 by 138 pixels). Otherwise,\naccess is denied automatically."}),"\n",(0,i.jsx)(n.p,{children:"Content running in the AIR application sandbox does not need the permission of\nthe user to access the microphone. Thus, status events for muting and unmuting\nthe microphone are never dispatched. Content running in AIR outside the\napplication sandbox does require permission from the user, so these status\nevents can be dispatched."}),"\n",(0,i.jsx)(n.h2,{id:"routing-microphone-audio-to-local-speakers",children:"Routing microphone audio to local speakers"}),"\n",(0,i.jsxs)(n.p,{children:["Audio input from a microphone can be routed to the local system speakers by\ncalling the ",(0,i.jsx)(n.code,{children:"Microphone.setLoopback()"})," method with a parameter value of ",(0,i.jsx)(n.code,{children:"true"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["When sound from a local microphone is routed to local speakers, there is a risk\nof creating an audio feedback loop, which can cause loud squealing sounds and\ncan potentially damage sound hardware. Calling the\n",(0,i.jsx)(n.code,{children:"Microphone.setUseEchoSuppression()"})," method with a parameter value of ",(0,i.jsx)(n.code,{children:"true"}),"\nreduces, but does not completely eliminate, the risk that audio feedback will\noccur. Adobe recommends you always call ",(0,i.jsx)(n.code,{children:"Microphone.setUseEchoSuppression(true)"}),"\nbefore calling ",(0,i.jsx)(n.code,{children:"Microphone.setLoopback(true)"}),", unless you are certain that the\nuser is playing back the sound using headphones or something other than\nspeakers."]}),"\n",(0,i.jsx)(n.p,{children:"The following code shows how to route the audio from a local microphone to the\nlocal system speakers:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"var mic:Microphone = Microphone.getMicrophone();\nmic.setUseEchoSuppression(true);\nmic.setLoopBack(true);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"altering-microphone-audio",children:"Altering microphone audio"}),"\n",(0,i.jsxs)(n.p,{children:["Your application can alter the audio data that comes from a microphone in two\nways. First, it can change the gain of the input sound, which effectively\nmultiplies the input values by a specified amount to create a louder or quieter\nsound. The ",(0,i.jsx)(n.code,{children:"Microphone.gain"})," property accepts numeric values between 0 and 100\ninclusive. A value of 50 acts like a multiplier of one and specifies normal\nvolume. A value of zero acts like a multiplier of zero and effectively silences\nthe input audio. Values above 50 specify higher than normal volume."]}),"\n",(0,i.jsxs)(n.p,{children:["Your application can also change the sample rate of the input audio. Higher\nsample rates increase sound quality, but they also create denser data streams\nthat use more resources for transmission and storage. The ",(0,i.jsx)(n.code,{children:"Microphone.rate"}),"\nproperty represents the audio sample rate measured in kilohertz (kHz). The\ndefault sample rate is 8 kHz. You can set the ",(0,i.jsx)(n.code,{children:"Microphone.rate"})," property to a\nvalue higher than 8 kHz if your microphone supports the higher rate. For\nexample, setting the ",(0,i.jsx)(n.code,{children:"Microphone.rate"})," property to a value of 11 sets the sample\nrate to 11 kHz; setting it to 22 sets the sample rate to 22 kHz, and so on. The\nsample rates available depend on the selected codec. When you use the Nellymoser\ncodec, you can specify 5, 8, 11, 16, 22 and 44 kHz as the sample rate. When you\nuse Speex codec (available starting in Flash Player 10 and Adobe AIR 1.5), you\ncan only use 16 kHz."]}),"\n",(0,i.jsx)(n.h2,{id:"detecting-microphone-activity",children:"Detecting microphone activity"}),"\n",(0,i.jsx)(n.p,{children:"To conserve bandwidth and processing resources, Flash Player tries to detect\nwhen no sound is being transmitted by a microphone. When the microphone's\nactivity level stays below the silence level threshold for a period of time,\nFlash Player stops transmitting the audio input and dispatches a simple\nActivityEvent instead. If you use the Speex codec (available in Flash Player 10\nor later and Adobe AIR 1.5 or later), set the silence level to 0, to ensure that\nthe application continuously transmits audio data. Speex voice activity\ndetection automatically reduces bandwidth."}),"\n",(0,i.jsxs)(n.p,{children:["Note: A Microphone object only dispatches Activity events when your application\nis monitoring the microphone. Thus, if you do not call ",(0,i.jsx)(n.code,{children:"setLoopBack( true )"}),",\nadd a listener for sample data events, or attach the microphone to a NetStream\nobject, then no activity events are dispatched."]}),"\n",(0,i.jsx)(n.p,{children:"Three properties of the Microphone class monitor and control the detection of\nactivity:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The read-only ",(0,i.jsx)(n.code,{children:"activityLevel"})," property indicates the amount of sound the\nmicrophone is detecting, on a scale from 0 to 100."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"silenceLevel"})," property specifies the amount of sound needed to activate\nthe microphone and dispatch an ",(0,i.jsx)(n.code,{children:"ActivityEvent.ACTIVITY"})," event. The\n",(0,i.jsx)(n.code,{children:"silenceLevel"})," property also uses a scale from 0 to 100, and the default value\nis 10."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"silenceTimeout"})," property describes the number of milliseconds that the\nactivity level must stay below the silence level, until an\n",(0,i.jsx)(n.code,{children:"ActivityEvent.ACTIVITY"})," event is dispatched to indicate that the microphone\nis now silent. The default ",(0,i.jsx)(n.code,{children:"silenceTimeout"})," value is 2000."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Both the ",(0,i.jsx)(n.code,{children:"Microphone.silenceLevel"})," property and the ",(0,i.jsx)(n.code,{children:"Microphone.silenceTimeout"}),"\nproperty are read only, but their values can be changed by using the\n",(0,i.jsx)(n.code,{children:"Microphone.setSilenceLevel()"})," method."]}),"\n",(0,i.jsxs)(n.p,{children:["In some cases, the process of activating the microphone when new activity is\ndetected can cause a short delay. Keeping the microphone active at all times can\nremove such activation delays. Your application can call the\n",(0,i.jsx)(n.code,{children:"Microphone.setSilenceLevel()"})," method with the ",(0,i.jsx)(n.code,{children:"silenceLevel"})," parameter set to\nzero to tell Flash Player to keep the microphone active and keep gathering audio\ndata, even when no sound is being detected. Conversely, setting the\n",(0,i.jsx)(n.code,{children:"silenceLevel"})," parameter to 100 prevents the microphone from being activated at\nall."]}),"\n",(0,i.jsx)(n.p,{children:"The following example displays information about the microphone and reports on\nactivity events and status events dispatched by a Microphone object:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'import flash.events.ActivityEvent;\nimport flash.events.StatusEvent;\nimport flash.media.Microphone;\n\nvar deviceArray:Array = Microphone.names;\ntrace("Available sound input devices:");\nfor (var i:int = 0; i < deviceArray.length; i++)\n{\n\ttrace(" " + deviceArray[i]);\n}\n\nvar mic:Microphone = Microphone.getMicrophone();\nmic.gain = 60;\nmic.rate = 11;\nmic.setUseEchoSuppression(true);\nmic.setLoopBack(true);\nmic.setSilenceLevel(5, 1000);\n\nmic.addEventListener(ActivityEvent.ACTIVITY, this.onMicActivity);\nmic.addEventListener(StatusEvent.STATUS, this.onMicStatus);\n\nvar micDetails:String = "Sound input device name: " + mic.name + \'\\n\';\nmicDetails += "Gain: " + mic.gain + \'\\n\';\nmicDetails += "Rate: " + mic.rate + " kHz" + \'\\n\';\nmicDetails += "Muted: " + mic.muted + \'\\n\';\nmicDetails += "Silence level: " + mic.silenceLevel + \'\\n\';\nmicDetails += "Silence timeout: " + mic.silenceTimeout + \'\\n\';\nmicDetails += "Echo suppression: " + mic.useEchoSuppression + \'\\n\';\ntrace(micDetails);\n\nfunction onMicActivity(event:ActivityEvent):void\n{\n\ttrace("activating=" + event.activating + ", activityLevel=" +\n\t\tmic.activityLevel);\n}\n\nfunction onMicStatus(event:StatusEvent):void\n{\n\ttrace("status: level=" + event.level + ", code=" + event.code);\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"When you run the above example, speak or makes noises into your system\nmicrophone and watch the resulting trace statements appear in a console or debug\nwindow."}),"\n",(0,i.jsx)(n.h2,{id:"sending-audio-to-and-from-a-media-server",children:"Sending audio to and from a media server"}),"\n",(0,i.jsx)(n.p,{children:"Additional audio capabilities are available when using ActionScript with a\nstreaming media server such as Flash Media Server."}),"\n",(0,i.jsx)(n.p,{children:"In particular, your application can attach a Microphone object to a NetStream\nobject and transmit data directly from the user's microphone to the server.\nAudio data can also be streamed from the server to an application and played\nback as part of a MovieClip or by using a Video object."}),"\n",(0,i.jsxs)(n.p,{children:["The Speex codec is available starting with Flash Player 10 and Adobe AIR 1.5. To\nset the codec used for compressed audio sent to the media server, set the\n",(0,i.jsx)(n.code,{children:"codec"})," property of the Microphone object. This property can have two values,\nwhich are enumerated in the SoundCodec class. Setting the codec property to\n",(0,i.jsx)(n.code,{children:"SoundCodec.SPEEX"})," selects the Speex codec for compressing audio. Setting the\nproperty to ",(0,i.jsx)(n.code,{children:"SoundCodec.NELLYMOSER"})," (the default) selects the Nellymoser codec\nfor compressing audio."]}),"\n",(0,i.jsxs)(n.p,{children:["For more information, see the Flash Media Server documentation online at\n",(0,i.jsx)(n.a,{href:"https://web.archive.org/web/20150702070954/http://www.adobe.com/support/documentation/en/flashmediaserver/",children:"www.adobe.com/go/learn_fms_docs_en"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"capturing-microphone-sound-data",children:"Capturing microphone sound data"}),"\n",(0,i.jsx)(n.p,{children:"In Flash Player 10.1 and AIR 2, or later, you can capture data from a microphone\ndata as a byte array of floating point values. Each value represents a sample of\nmonophonic audio data."}),"\n",(0,i.jsxs)(n.p,{children:["To get microphone data, set an event listener for the ",(0,i.jsx)(n.code,{children:"sampleData"})," event of the\nMicrophone object. The Microphone object dispatches ",(0,i.jsx)(n.code,{children:"sampleData"})," events\nperiodically as the microphone buffer is filled with sound samples. The\nSampleDataEvent object has a ",(0,i.jsx)(n.code,{children:"data"})," property that is a byte array of sound\nsamples. The samples are each represented as floating point values, each\nrepresenting a monophonic sound sample."]}),"\n",(0,i.jsxs)(n.p,{children:["The following code captures microphone sound data into a ByteArray object named\n",(0,i.jsx)(n.code,{children:"soundBytes"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"var mic:Microphone = Microphone.getMicrophone();\nmic.setSilenceLevel(0, DELAY_LENGTH);\nmic.addEventListener(SampleDataEvent.SAMPLE_DATA, micSampleDataHandler);\nfunction micSampleDataHandler(event:SampleDataEvent):void {\n\twhile(event.data.bytesAvailable) {\n\t\tvar sample:Number = event.data.readFloat();\n\t\tsoundBytes.writeFloat(sample);\n\t}\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can reuse the sample bytes as playback audio for a Sound object. If you do,\nyou should set the ",(0,i.jsx)(n.code,{children:"rate"})," property of the Microphone object to 44, which is the\nsample rate used by Sound objects. (You can also convert microphone samples\ncaptured at a lower rate to 44 kHz rate required by the Sound object.) Also,\nkeep in mind that the Microphone object captures monophonic samples, whereas the\nSound object uses stereo sound; so you should write each of the bytes captured\nby the Microphone object to the Sound object twice. The following example\ncaptures 4 seconds of microphone data and plays it back using a Sound object:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'const DELAY_LENGTH:int = 4000;\nvar mic:Microphone = Microphone.getMicrophone();\nmic.setSilenceLevel(0, DELAY_LENGTH);\nmic.gain = 100;\nmic.rate = 44;\nmic.addEventListener(SampleDataEvent.SAMPLE_DATA, micSampleDataHandler);\n\nvar timer:Timer = new Timer(DELAY_LENGTH);\ntimer.addEventListener(TimerEvent.TIMER, timerHandler);\ntimer.start();\n\nfunction micSampleDataHandler(event:SampleDataEvent):void\n{\n\twhile(event.data.bytesAvailable)\n\t{\n\t\tvar sample:Number = event.data.readFloat();\n\t\tsoundBytes.writeFloat(sample);\n\t}\n}\nvar sound:Sound = new Sound();\nvar channel:SoundChannel;\nfunction timerHandler(event:TimerEvent):void\n{\n\tmic.removeEventListener(SampleDataEvent.SAMPLE_DATA, micSampleDataHandler);\n\ttimer.stop();\n\tsoundBytes.position = 0;\n\tsound.addEventListener(SampleDataEvent.SAMPLE_DATA, playbackSampleHandler);\n\tchannel.addEventListener( Event.SOUND_COMPLETE, playbackComplete );\n\tchannel = sound.play();\n}\n\nfunction playbackSampleHandler(event:SampleDataEvent):void\n{\n\tfor (var i:int = 0; i < 8192 && soundBytes.bytesAvailable > 0; i++)\n\t{\n\t\ttrace(sample);\n\t\tvar sample:Number = soundBytes.readFloat();\n\t\tevent.data.writeFloat(sample);\n\t\tevent.data.writeFloat(sample);\n\t}\n}\n\nfunction playbackComplete( event:Event ):void\n{\n\ttrace( "Playback finished.");\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["For more information on playing back sounds from sound sample data, see\n",(0,i.jsx)(n.a,{href:"/docs/development/rich-media-content/working-with-sound/working-with-dynamically-generated-audio",children:"Working with dynamically generated audio"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"More Help topics"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://web.archive.org/web/20150120163947/http://www.riagora.com/2010/08/air-android-and-the-microphone/",children:"Michael Chaize: AIR, Android, and the Microphone"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://web.archive.org/web/20120913011517/http://coenraets.org/blog/air-for-android-samples/voice-notes-for-android/",children:"Christophe Coenraets: Voice Notes for Android"})})]})}function l(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);