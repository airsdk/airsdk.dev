"use strict";(self.webpackChunkairsdk_dev=self.webpackChunkairsdk_dev||[]).push([[90788],{28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>s});var i=n(96540);const a={},d=i.createContext(a);function r(e){const t=i.useContext(d);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(d.Provider,{value:t},e.children)}},77195:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"development/rich-media-content/working-with-video/using-cue-points-and-metadata","title":"Using cue points and metadata","description":"Use the NetStream callback methods to capture and process cue point and metadata","source":"@site/docs/development/rich-media-content/working-with-video/using-cue-points-and-metadata.md","sourceDirName":"development/rich-media-content/working-with-video","slug":"/development/rich-media-content/working-with-video/using-cue-points-and-metadata","permalink":"/docs/development/rich-media-content/working-with-video/using-cue-points-and-metadata","draft":false,"unlisted":false,"editUrl":"https://github.com/airsdk/airsdk.dev/edit/main/docs/development/rich-media-content/working-with-video/using-cue-points-and-metadata.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10},"sidebar":"mainSidebar","previous":{"title":"Writing callback methods for metadata and cue points","permalink":"/docs/development/rich-media-content/working-with-video/writing-callback-methods-for-metadata-and-cue-points"},"next":{"title":"Monitoring NetStream activity","permalink":"/docs/development/rich-media-content/working-with-video/monitoring-netstream-activity"}}');var a=n(74848),d=n(28453);const r={sidebar_position:10},s="Using cue points and metadata",o={},c=[{value:"Using cue points",id:"using-cue-points",level:2},{value:"Using video metadata",id:"using-video-metadata",level:2},{value:"Using OnMetaData()",id:"using-onmetadata",level:3},{value:"Using the information object",id:"using-the-information-object",level:4},{value:"Using onXMPData()",id:"using-onxmpdata",level:3},{value:"Using image metadata",id:"using-image-metadata",level:2},{value:"Using text metadata",id:"using-text-metadata",level:2}];function l(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"using-cue-points-and-metadata",children:"Using cue points and metadata"})}),"\n",(0,a.jsx)(t.p,{children:"Use the NetStream callback methods to capture and process cue point and metadata\nevents as the video plays."}),"\n",(0,a.jsx)(t.h2,{id:"using-cue-points",children:"Using cue points"}),"\n",(0,a.jsx)(t.p,{children:"The following table describes the callback methods that you can use to capture\nF4V and FLV cue points in Flash Player and AIR."}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Runtime"}),(0,a.jsx)(t.th,{children:"F4V"}),(0,a.jsx)(t.th,{children:"FLV"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"Flash Player 9/ AIR1.0"}),(0,a.jsx)(t.td,{}),(0,a.jsx)(t.td,{children:"OnCuePoint"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{}),(0,a.jsx)(t.td,{}),(0,a.jsx)(t.td,{children:"OnMetaData"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"Flash Player 10"}),(0,a.jsx)(t.td,{}),(0,a.jsx)(t.td,{children:"OnCuePoint"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{}),(0,a.jsx)(t.td,{children:"OnMetaData"}),(0,a.jsx)(t.td,{children:"OnMetaData"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{}),(0,a.jsx)(t.td,{children:"OnXMPData"}),(0,a.jsx)(t.td,{children:"OnXMPData"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:["The following example uses a simple ",(0,a.jsx)(t.code,{children:"for..in"})," loop to iterate over each of the\nproperties in the ",(0,a.jsx)(t.code,{children:"infoObject"})," parameter that the ",(0,a.jsx)(t.code,{children:"onCuePoint()"})," function\nreceives. It calls the trace() function to display a message when it receives\ncue point data:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'var nc:NetConnection = new NetConnection();\nnc.connect(null);\n\nvar ns:NetStream = new NetStream(nc);\nns.client = this;\nns.play("video.flv");\n\nvar vid:Video = new Video();\nvid.attachNetStream(ns);\naddChild(vid);\n\nfunction onCuePoint(infoObject:Object):void\n{\n\tvar key:String;\n\tfor (key in infoObject)\n\t{\n\t\ttrace(key + ": " + infoObject[key]);\n\t}\n}\n'})}),"\n",(0,a.jsx)(t.p,{children:"The following output appears:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"parameters:\nname: point1\ntime: 0.418\ntype: navigation\n"})}),"\n",(0,a.jsxs)(t.p,{children:["This code uses one of several techniques to set the object on which the callback\nmethod runs. You can use other techniques; for more information, see\n",(0,a.jsx)(t.a,{href:"/docs/development/rich-media-content/working-with-video/writing-callback-methods-for-metadata-and-cue-points",children:"Writing callback methods for metadata and cue points"}),"."]}),"\n",(0,a.jsx)(t.h2,{id:"using-video-metadata",children:"Using video metadata"}),"\n",(0,a.jsxs)(t.p,{children:["You can use the ",(0,a.jsx)(t.code,{children:"OnMetaData()"})," and ",(0,a.jsx)(t.code,{children:"OnXMPData()"})," functions to access the\nmetadata information in your video file, including cue points."]}),"\n",(0,a.jsx)(t.h3,{id:"using-onmetadata",children:"Using OnMetaData()"}),"\n",(0,a.jsx)(t.p,{children:"Metadata includes information about your video file, such as duration, width,\nheight, and frame rate. The metadata information that is added to your video\nfile depends on the software you use to encode the video file."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'var nc:NetConnection = new NetConnection();\nnc.connect(null);\n\nvar ns:NetStream = new NetStream(nc);\nns.client = this;\nns.play("video.flv");\n\nvar vid:Video = new Video();\nvid.attachNetStream(ns);\naddChild(vid);\n\nfunction onMetaData(infoObject:Object):void\n{\n\tvar key:String;\n\tfor (key in infoObject)\n\t{\n\t\ttrace(key + ": " + infoObject[key]);\n\t}\n}\n'})}),"\n",(0,a.jsx)(t.p,{children:"The previous code generates output like the following:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"width: 320\naudiodelay: 0.038\ncanSeekToEnd: true\nheight: 213\ncuePoints: ,,\naudiodatarate: 96\nduration: 16.334\nvideodatarate: 400\nframerate: 15\nvideocodecid: 4\naudiocodecid: 2\n"})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.img,{src:n(81029).A+"",width:"17",height:"21"})," If your video does not have audio, the audio-related\nmetadata information (such as ",(0,a.jsx)(t.code,{children:"audiodatarate"}),") returns ",(0,a.jsx)(t.code,{children:"undefined"})," ",(0,a.jsx)(t.em,{children:"because no\naudio information is added to the metadata during encoding."})]}),"\n",(0,a.jsx)(t.p,{children:"In the previous code, the cue point information was not displaying. To display\nthe cue point metadata, you can use the following function which recursively\ndisplays the items in an Object:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'function traceObject(obj:Object, indent:uint = 0):void\n{\n\tvar indentString:String = "";\n\tvar i:uint;\n\tvar prop:String;\n\tvar val:*;\n\tfor (i = 0; i < indent; i++)\n\t{\n\t\tindentString += "\\t";\n\t}\n\tfor (prop in obj)\n\t{\n\t\tval = obj[prop];\n\t\tif (typeof(val) == "object")\n\t\t{\n\t\t\ttrace(indentString + " " + prop + ": [Object]");\n\t\t\ttraceObject(val, indent + 1);\n\t\t}\n\t\telse\n\t\t{\n\t\t\ttrace(indentString + " " + prop + ": " + val);\n\t\t}\n\t}\n}\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Using the previous code snippet to trace the ",(0,a.jsx)(t.code,{children:"infoObject"})," parameter in the\n",(0,a.jsx)(t.code,{children:"onMetaData()"})," method creates the following output:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"width: 320\naudiodatarate: 96\naudiocodecid: 2\nvideocodecid: 4\nvideodatarate: 400\ncanSeekToEnd: true\nduration: 16.334\naudiodelay: 0.038\nheight: 213\nframerate: 15\ncuePoints: [Object]\n0: [Object]\n\tparameters: [Object]\n\t\tlights: beginning\n\tname: point1\n\ttime: 0.418\n\ttype: navigation\n1: [Object]\n\tparameters: [Object]\n\t\tlights: middle\n\tname: point2\n\ttime: 7.748\n\ttype: navigation\n2: [Object]\n\tparameters: [Object]\n\t\tlights: end\n\tname: point3\n\ttime: 16.02\n\ttype: navigation\n"})}),"\n",(0,a.jsxs)(t.p,{children:["The following example displays the metadata for an MP4 video. It assumes that\nthere is a TextArea object called ",(0,a.jsx)(t.code,{children:"metaDataOut"}),", to which it writes the\nmetadata."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'package\n{\n\timport flash.net.NetConnection;\n\timport flash.net.NetStream;\n\timport flash.events.NetStatusEvent;\n\timport flash.media.Video;\n\timport flash.display.StageDisplayState;\n\timport flash.display.Loader;\n\timport flash.display.Sprite;\n\timport flash.events.MouseEvent;\n\n\tpublic class onMetaDataExample extends Sprite\n\t{\n\t\tvar video:Video = new Video();\n\n\t\tpublic function onMetaDataExample():void\n\t\t{\n\t\t\tvar videoConnection:NetConnection = new NetConnection();\n\t\t\tvideoConnection.connect(null);\n\n\t\t\tvar videoStream:NetStream = new NetStream(videoConnection);\n\t\t\tvideoStream.client = this;\n\n\t\t\taddChild(video);\n\t\t\tvideo.x = 185;\n\t\t\tvideo.y = 5;\n\n\t\t\tvideo.attachNetStream(videoStream);\n\n\t\t\tvideoStream.play("video.mp4");\n\n\t\t\tvideoStream.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\t\t}\n\n\t\tpublic function onMetaData(infoObject:Object):void\n\t\t{\n\t\t\tfor(var propName:String in infoObject)\n\t\t\t{\n\t\t\t\tmetaDataOut.appendText(propName + "=" + infoObject[propName] + "\\n");\n\t\t\t}\n\t\t}\n\n\t\tprivate function netStatusHandler(event:NetStatusEvent):void\n\t\t{\n\t\t\tif(event.info.code == "NetStream.Play.Stop")\n\t\t\t\tstage.displayState = StageDisplayState.NORMAL;\n\t\t}\n\t}\n}\n'})}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"onMetaData()"})," function produced the following output for this video:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"moovposition=731965\nheight=352\navclevel=21\nvideocodecid=avc1\nduration=2.36\nwidth=704\nvideoframerate=25\navcprofile=88\ntrackinfo=[object Object]\n"})}),"\n",(0,a.jsx)(t.h4,{id:"using-the-information-object",children:"Using the information object"}),"\n",(0,a.jsxs)(t.p,{children:["The following table shows the possible values for video metadata that are passed\nto the ",(0,a.jsx)(t.code,{children:"onMetaData()"})," callback function in the Object that they receive:"]}),"\n",(0,a.jsxs)("table",{children:[(0,a.jsx)("thead",{children:(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:(0,a.jsx)("p",{children:"Parameter"})}),(0,a.jsx)("th",{children:(0,a.jsx)("p",{children:"Description"})})]})}),(0,a.jsxs)("tbody",{children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"aacaot"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"AAC audio object type; 0, 1, or 2 are supported."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"avclevel"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"AVC IDC level number such as 10, 11, 20, 21, and so on."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"avcprofile"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"AVC profile number such as 55, 77, 100, and so on."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"audiocodecid"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:'A string that indicates the audio codec (code/decode technique) that\nwas used - for example ".mp3" or "mp4a"'})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"audiodatarate"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:"A number that indicates the rate at which audio was encoded, in\nkilobytes per second."})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"audiodelay"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:'A number that indicates what time in the FLV file "time 0" of the\noriginal FLV file exists. The video content needs to be delayed by a\nsmall amount to properly synchronize the audio.'})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"canSeekToEnd"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsxs)(t.p,{children:["A Boolean value that is ",(0,a.jsx)("samp",{children:"true"})," if the FLV file is encoded\nwith a keyframe on the last frame, which allows seeking to the end of\na progressive -download video file. It is ",(0,a.jsx)("samp",{children:"false"})," if the\nFLV file is not encoded with a keyframe on the last frame."]})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"cuePoints"})}),(0,a.jsxs)("td",{children:[(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:"An array of objects, one for each cue point embedded in the FLV file.\nValue is undefined if the FLV file does not contain any cue points.\nEach object has the following properties:"})}),(0,a.jsxs)("ul",{class:"incremental",children:[(0,a.jsx)("li",{children:(0,a.jsx)("p",{children:(0,a.jsxs)(t.p,{children:[(0,a.jsx)("samp",{children:"type"}),': a string that specifies the type of cue point\nas either "navigation" or "event".']})})}),(0,a.jsx)("li",{children:(0,a.jsx)("p",{children:(0,a.jsxs)(t.p,{children:[(0,a.jsx)("samp",{children:"name"}),": a string that is the name of the cue point."]})})}),(0,a.jsx)("li",{children:(0,a.jsx)("p",{children:(0,a.jsxs)(t.p,{children:[(0,a.jsx)("samp",{children:"time"}),": a number that is the time of the cue point in\nseconds with a precision of three decimal places (milliseconds)."]})})}),(0,a.jsx)("li",{children:(0,a.jsx)("p",{children:(0,a.jsxs)(t.p,{children:[(0,a.jsx)("samp",{children:"parameters"}),": an optional object that has name-value\npairs that are designated by the user when creating the cue\npoints."]})})})]})]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"duration"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:"A number that specifies the duration of the video file, in seconds."})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"framerate"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"A number that is the frame rate of the FLV file."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"height"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"A number that is the height of the FLV file, in pixels."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"seekpoints"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:"An array that lists the available keyframes as timestamps in\nmilliseconds. Optional."})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"tags"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:'An array of key-value pairs that represent the information in the\n"ilst" atom, which is the equivalent of ID3 tags for MP4 files. iTunes\nuses these tags. Can be used to display artwork, if available.'})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"trackinfo"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:(0,a.jsx)(t.p,{children:"Object that provides information on all the tracks in the MP4 file,\nincluding their sample description ID."})})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"videocodecid"})}),(0,a.jsx)("td",{children:(0,a.jsxs)("p",{children:[(0,a.jsx)(t.p,{children:"A string that is the codec version that was used to encode the video."}),(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:'for example, "avc1" or "VP6F"'}),"\n"]})]})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"videodatarate"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"A number that is the video data rate of the FLV file."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"videoframerate"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"Framerate of the MP4 video."})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"width"})}),(0,a.jsx)("td",{children:(0,a.jsx)("p",{children:"A number that is the width of the FLV file, in pixels."})})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:["The following table shows the possible values for the ",(0,a.jsx)(t.code,{children:"videocodecid"})," parameter:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"videocodecid"}),(0,a.jsx)(t.th,{children:"Codec name"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"2"}),(0,a.jsx)(t.td,{children:"Sorenson H.263"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"3"}),(0,a.jsx)(t.td,{children:"Screen video (SWF version 7 and later only)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"4"}),(0,a.jsx)(t.td,{children:"VP6 (SWF version 8 and later only)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"5"}),(0,a.jsx)(t.td,{children:"VP6 video with alpha channel (SWF version 8 and later only)"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:["The following table shows the possible values for the ",(0,a.jsx)(t.code,{children:"audiocodecid"})," parameter:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"audiocodecid"}),(0,a.jsx)(t.th,{children:"Codec Name"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"0"}),(0,a.jsx)(t.td,{children:"uncompressed"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"1"}),(0,a.jsx)(t.td,{children:"ADPCM"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"2"}),(0,a.jsx)(t.td,{children:"Mp3"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"4"}),(0,a.jsx)(t.td,{children:"Nellymoser @ 16 kHz mono"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"5"}),(0,a.jsx)(t.td,{children:"Nellymoser, 8kHz mono"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"6"}),(0,a.jsx)(t.td,{children:"Nellymoser"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"10"}),(0,a.jsx)(t.td,{children:"AAC"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"11"}),(0,a.jsx)(t.td,{children:"Speex"})]})]})]}),"\n",(0,a.jsx)(t.h3,{id:"using-onxmpdata",children:"Using onXMPData()"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"onXMPData()"})," callback function receives information specific to Adobe\nExtensible Metadata Platform (XMP) that is embedded in the Adobe F4V or FLV\nvideo file. The XMP metadata includes cue points as well as other video\nmetadata. XMP metadata support is introduced with Flash Player 10 and Adobe AIR\n1.5 and supported by subsequent versions."]}),"\n",(0,a.jsx)(t.p,{children:"The following example processes cue point data in the XMP metadata:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'package\n{\n\timport flash.display.*;\n\timport flash.net.*;\n\timport flash.events.NetStatusEvent;\n\timport flash.media.Video;\n\n\tpublic class onXMPDataExample extends Sprite\n\t{\n\t\tpublic function onXMPDataExample():void\n\t\t{\n\t\t\tvar videoConnection:NetConnection = new NetConnection();\n\t\t\tvideoConnection.connect(null);\n\n\t\t\tvar videoStream:NetStream = new NetStream(videoConnection);\n\t\t\tvideoStream.client = this;\n\t\t\tvar video:Video = new Video();\n\n\t\t\taddChild(video);\n\n\t\t\tvideo.attachNetStream(videoStream);\n\n\t\t\tvideoStream.play("video.f4v");\n\t\t}\n\n\t\tpublic function onMetaData(info:Object):void {\n\t\t\ttrace("onMetaData fired");\n\t\t}\n\n\t\tpublic function onXMPData(infoObject:Object):void\n\t\t{\n\t\t\ttrace("onXMPData Fired\\n");\n\t\t\t//trace("raw XMP =\\n");\n\t\t\t//trace(infoObject.data);\n\t\t\tvar cuePoints:Array = new Array();\n\t\t\tvar cuePoint:Object;\n\t\t\tvar strFrameRate:String;\n\t\t\tvar nTracksFrameRate:Number;\n\t\t\tvar strTracks:String = "";\n\t\t\tvar onXMPXML = new XML(infoObject.data);\n\t\t\t// Set up namespaces to make referencing easier\n\t\t\tvar xmpDM:Namespace = new Namespace("http://ns.adobe.com/xmp/1.0/DynamicMedia/");\n\t\t\tvar rdf:Namespace = new Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#");\n\t\t\tfor each (var it:XML in onXMPXML..xmpDM::Tracks)\n\t\t\t{\n\t\t\t\tvar strTrackName:String = it.rdf::Bag.rdf::li.rdf::Description.@xmpDM::trackName;\n\t\t\t\tvar strFrameRateXML:String = it.rdf::Bag.rdf::li.rdf::Description.@xmpDM::frameRate;\n\t\t\t\tstrFrameRate = strFrameRateXML.substr(1,strFrameRateXML.length);\n\n\t\t\t\tnTracksFrameRate = Number(strFrameRate);\n\n\t\t\t\tstrTracks += it;\n\t\t\t}\n\t\t\tvar onXMPTracksXML:XML = new XML(strTracks);\n\t\t\tvar strCuepoints:String = "";\n\t\t\tfor each (var item:XML in onXMPTracksXML..xmpDM::markers)\n\t\t\t{\n\t\t\t\tstrCuepoints += item;\n\t\t\t}\n\t\t\ttrace(strCuepoints);\n\t\t}\n\t}\n}\n'})}),"\n",(0,a.jsx)(t.p,{children:"For a short video file called startrekintro.f4v, this example produces the\nfollowing trace lines. The lines show the cue point data for navigation and\nevent cue points in the XMP meta data:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'onMetaData fired\nonXMPData Fired\n\n<xmpDM:markers xmlns:xmp="http://ns.adobe.com/xap/1.0/" xmlns:xmpDM="http://ns.adobe.com/xmp/1.0/DynamicMedia/" xmlns:stDim="http://ns.adobe.com/xap/1.0/sType/Dimensions#" xmlns:xmpMM="http://ns.adobe.com/xap/1.0/mm/" xmlns:stEvt="http://ns.adobe.com/xap/1.0/sType/ResourceEvent#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:x="adobe:ns:meta/">\n<rdf:Seq>\n<rdf:li>\n\t<rdf:Description xmpDM:startTime="7695905817600" xmpDM:name="Title1" xmpDM:type="FLVCuePoint" xmpDM:cuePointType="Navigation">\n\t<xmpDM:cuePointParams>\n\t\t<rdf:Seq>\n\t\t<rdf:li xmpDM:key="Title" xmpDM:value="Star Trek"/>\n\t\t<rdf:li xmpDM:key="Color" xmpDM:value="Blue"/>\n\t\t</rdf:Seq>\n\t</xmpDM:cuePointParams>\n\t</rdf:Description>\n</rdf:li>\n<rdf:li>\n\t<rdf:Description xmpDM:startTime="10289459980800" xmpDM:name="Title2" xmpDM:type="FLVCuePoint" xmpDM:cuePointType="Event">\n\t<xmpDM:cuePointParams>\n\t\t<rdf:Seq>\n\t\t<rdf:li xmpDM:key="William Shatner" xmpDM:value="First Star"/>\n\t\t<rdf:li xmpDM:key="Color" xmpDM:value="Light Blue"/>\n\t\t</rdf:Seq>\n\t</xmpDM:cuePointParams>\n\t</rdf:Description>\n</rdf:li>\n</rdf:Seq>\n</xmpDM:markers>\nonMetaData fired\n'})}),"\n",(0,a.jsx)(t.p,{children:"Note: In XMP data, time is stored as DVA Ticks rather than seconds. To compute\nthe cue point time, divide the start time by the framerate. For example, the\nstart time of 7695905817600 divided by a framerate of 254016000000 equals 30:30."}),"\n",(0,a.jsxs)(t.p,{children:["To see the complete raw XMP metadata, which includes the framerate, remove the\ncomment identifiers (//'s) preceding the second and third ",(0,a.jsx)(t.code,{children:"trace()"})," statements\nat the beginning of the ",(0,a.jsx)(t.code,{children:"onXMPData()"})," function."]}),"\n",(0,a.jsx)(t.p,{children:"For more information on XMP, see:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://web.archive.org/web/20080304172551/http://partners.adobe.com/public/developer/xmp/topic.html",children:"Adobe Extensible Metadata Platform (XMP)"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://web.archive.org/web/20140302023642/http://www.adobe.com/devnet/xmp.html",children:"Adobe XMP Developer Center"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"using-image-metadata",children:"Using image metadata"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"onImageData"})," event sends image data as a byte array through an AMF0 data\nchannel. The data can be in JPEG, PNG, or GIF formats. Define an ",(0,a.jsx)(t.code,{children:"onImageData()"}),"\ncallback method to process this information, in the same way that you would\ndefine callback methods for ",(0,a.jsx)(t.code,{children:"onCuePoint"})," and ",(0,a.jsx)(t.code,{children:"onMetaData"}),". The following example\naccesses and displays image data using an ",(0,a.jsx)(t.code,{children:"onImageData()"})," callback method:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"public function onImageData(imageData:Object):void\n{\n\t// display track number\n\ttrace(imageData.trackid);\n\tvar loader:Loader = new Loader();\n\t//imageData.data is a ByteArray object\n\tloader.loadBytes(imageData.data);\n\taddChild(loader);\n}\n"})}),"\n",(0,a.jsx)(t.h2,{id:"using-text-metadata",children:"Using text metadata"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"onTextData"})," event sends text data through an AMF0 data channel. The text\ndata is in UTF-8 format and contains additional information about formatting,\nbased on the 3GP timed-text specification. This specification defines a\nstandardized subtitle format. Define an ",(0,a.jsx)(t.code,{children:"onTextData()"})," callback method to\nprocess this information, in the same way that you would define callback methods\nfor ",(0,a.jsx)(t.code,{children:"onCuePoint"})," or ",(0,a.jsx)(t.code,{children:"onMetaData"}),". In the following example, the ",(0,a.jsx)(t.code,{children:"onTextData()"}),"\nmethod displays the track ID number and corresponding track text."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"public function onTextData(textData:Object):void\n{\n\t// display the track number\n\ttrace(textData.trackid);\n\t// displays the text, which can be a null string, indicating old text\n\t// that should be erased\n\ttrace(textData.text);\n}\n"})})]})}function h(e={}){const{wrapper:t}={...(0,d.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},81029:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/tip_help-3d4c326bf05518d713f5bf1c3f47bcde.png"}}]);