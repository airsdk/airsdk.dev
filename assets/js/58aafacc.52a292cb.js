"use strict";(self.webpackChunkairsdk_dev=self.webpackChunkairsdk_dev||[]).push([[22611],{15789:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"development/user-interaction/touch-multitouch-and-gesture-input/basics-of-touch-input","title":"Basics of touch input","description":"When the Flash Platform is running in an environment that supports touch input,","source":"@site/docs/development/user-interaction/touch-multitouch-and-gesture-input/basics-of-touch-input.md","sourceDirName":"development/user-interaction/touch-multitouch-and-gesture-input","slug":"/development/user-interaction/touch-multitouch-and-gesture-input/basics-of-touch-input","permalink":"/docs/development/user-interaction/touch-multitouch-and-gesture-input/basics-of-touch-input","draft":false,"unlisted":false,"editUrl":"https://github.com/airsdk/airsdk.dev/edit/main/docs/development/user-interaction/touch-multitouch-and-gesture-input/basics-of-touch-input.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"mainSidebar","previous":{"title":"Touch, multitouch and gesture input","permalink":"/docs/development/user-interaction/touch-multitouch-and-gesture-input/"},"next":{"title":"Touch support discovery","permalink":"/docs/development/user-interaction/touch-multitouch-and-gesture-input/touch-support-discovery"}}');var o=n(74848),s=n(28453);const r={sidebar_position:1},a="Basics of touch input",c={},u=[{value:"Important concepts and terms",id:"important-concepts-and-terms",level:4},{value:"The touch input API structure",id:"the-touch-input-api-structure",level:2},{value:"Discovery",id:"discovery",level:3},{value:"Events",id:"events",level:3},{value:"Phases",id:"phases",level:3}];function h(e){const t={a:"a",br:"br",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"basics-of-touch-input",children:"Basics of touch input"})}),"\n",(0,o.jsxs)(t.p,{children:["When the Flash Platform is running in an environment that supports touch input,\nInteractiveObject instances can listen for touch events and call handlers.\nGenerally, you handle touch, multitouch, and gesture events as you would other\nevents in ActionScript (see\n",(0,o.jsx)(t.a,{href:"/docs/development/core-actionscript-classes/handling-events/",children:"Handling events"})," for\nbasic information about event handling with ActionScript)."]}),"\n",(0,o.jsxs)(t.p,{children:["However, for the Flash runtime to interpret a touch or gesture, the runtime must\nbe running in a hardware and software environment that supports touch or\nmultitouch input. See\n",(0,o.jsx)(t.a,{href:"/docs/development/user-interaction/basics-of-user-interaction#discovering-input-types",children:"Discovering input types"}),"\nfor a chart comparing different touch screen types. Additionally, if the runtime\nis running within a container application (such as a browser), then that\ncontainer passes the input to the runtime. In some cases, the current hardware\nand operating system environment support multitouch, but the browser containing\nthe Flash runtime interprets the input and does not pass it on to the runtime.\nOr, it can simply ignore the input altogether."]}),"\n",(0,o.jsx)(t.p,{children:"The following diagram shows the flow of input from user to runtime:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:n(47044).A+"",width:"610",height:"117"})}),"\n",(0,o.jsx)(t.p,{children:"Flow of input from user to the Flash Platform runtime"}),"\n",(0,o.jsx)(t.p,{children:'Fortunately, the ActionScript API for developing touch applications includes\nclasses, methods, and properties to determine the support for touch or\nmultitouch input in the runtime environment. The API you use to determine\nsupport for touch input are the "discovery API" for touch event handling.'}),"\n",(0,o.jsx)(t.h4,{id:"important-concepts-and-terms",children:"Important concepts and terms"}),"\n",(0,o.jsx)(t.p,{children:"The following reference list contains important terms for writing touch\nevent-handling applications:"}),"\n",(0,o.jsxs)(t.p,{children:["Discovery API",(0,o.jsx)(t.br,{}),"\n","The methods and properties used to test the runtime environment for support of\ntouch events and different modes of input."]}),"\n",(0,o.jsxs)(t.p,{children:["Touch event",(0,o.jsx)(t.br,{}),"\n","An input action performed on a touch-enabled device using a single point of\ncontact."]}),"\n",(0,o.jsxs)(t.p,{children:["Touch point",(0,o.jsx)(t.br,{}),"\n","The point of contact for a single touch event. Even if a device does not support\ngesture input, it might support multiple simultaneous touch points."]}),"\n",(0,o.jsxs)(t.p,{children:["Touch sequence",(0,o.jsx)(t.br,{}),"\n","The series of events representing the lifespan of a single touch. These events\ninclude one beginning, zero or more moves, and one end."]}),"\n",(0,o.jsxs)(t.p,{children:["Multitouch event",(0,o.jsx)(t.br,{}),"\n","An input action performed on a touch-enabled device using several points of\ncontact (such as more than one finger)."]}),"\n",(0,o.jsxs)(t.p,{children:["Gesture event",(0,o.jsx)(t.br,{}),"\n","An input action performed on a touch-enabled device tracing some complex\nmovement. For example, one gesture is touching a screen with two fingers and\nmoving them simultaneously around the perimeter of an abstract circle to\nindicate rotation."]}),"\n",(0,o.jsxs)(t.p,{children:["Phases",(0,o.jsx)(t.br,{}),"\n","Distinct points of time in the event flow (such as begin and end)."]}),"\n",(0,o.jsxs)(t.p,{children:["Stylus",(0,o.jsx)(t.br,{}),"\n","An instrument for interacting with a touch-enabled screen. A stylus can provide\nmore precision than the human finger. Some devices recognize only input from a\nspecific type of stylus. Devices that do recognize stylus input might not\nrecognize multiple, simultaneous points of contact or finger contact."]}),"\n",(0,o.jsxs)(t.p,{children:["Press-and-tap",(0,o.jsx)(t.br,{}),"\n","A specific type of multitouch input gesture where the user pushes a finger\nagainst a touch-enabled device and then taps with another finger or pointing\ndevice. This gesture is often used to simulate a mouse right-click in multitouch\napplications."]}),"\n",(0,o.jsx)(t.h2,{id:"the-touch-input-api-structure",children:"The touch input API structure"}),"\n",(0,o.jsx)(t.p,{children:"The ActionScript touch input API is designed to address the fact that touch\ninput handling depends on the hardware and software environment of the Flash\nruntime. The touch input API primarily addresses three needs of touch\napplication development: discovery, events, and phases. Coordinate these API to\nproduce a predictable and responsive experience for the user; even if the target\ndevice is unknown as you develop an application."}),"\n",(0,o.jsx)(t.h3,{id:"discovery",children:"Discovery"}),"\n",(0,o.jsxs)(t.p,{children:["The discovery API provides the ability to test the hardware and software\nenvironment at runtime. The values populated by the runtime determine the touch\ninput available to the Flash runtime in its current context. Also, use the\ncollection of discovery properties and methods to set your application to react\nto mouse events (instead of touch events in case some touch input is not\nsupported by the environment). For more information, see\n",(0,o.jsx)(t.a,{href:"/docs/development/user-interaction/touch-multitouch-and-gesture-input/touch-support-discovery",children:"Touch support discovery"}),"."]}),"\n",(0,o.jsx)(t.h3,{id:"events",children:"Events"}),"\n",(0,o.jsx)(t.p,{children:"ActionScript manages touch input events with event listeners and event handlers\nas it does other events. However, touch input event handling also must take into\naccount:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"A touch can be interpreted in several ways by the device or operating system,\neither as a sequence of touches or, collectively, as a gesture."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"A single touch to a touch-enabled device (by a finger, stylus or pointing\ndevice) always dispatches a mouse event, too. You can handle the mouse event\nwith the event types in the MouseEvent class. Or, you can design your\napplication only to respond to touch events. Or, you can design an application\nthat responds to both."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"An application can respond to multiple, simultaneous touch events, and handle\neach one separately."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(t.p,{children:["Typically, use the discovery API to conditionally handle the events your\napplication handles, and how they are handled. Once the application knows the\nruntime environment, it can call the appropriate handler or establish the\ncorrect event object when the user interacts with the application. Or, the\napplication can indicate that specific input cannot be handled in the current\nenvironment and provide the user with an alternative or information. For more\ninformation, see ",(0,o.jsx)(t.a,{href:"/docs/development/user-interaction/touch-multitouch-and-gesture-input/touch-event-handling",children:"Touch event handling"})," and\n",(0,o.jsx)(t.a,{href:"/docs/development/user-interaction/touch-multitouch-and-gesture-input/gesture-event-handling",children:"Gesture event handling"}),"."]}),"\n",(0,o.jsx)(t.h3,{id:"phases",children:"Phases"}),"\n",(0,o.jsx)(t.p,{children:"For touch and multitouch applications, touch event objects contain properties to\ntrack the phases of user interaction. Write ActionScript to handle phases like\nthe begin, update, or end phase of user input to provide the user with feedback.\nRespond to event phases so visual objects change as the user touch and moves the\npoint of touch on a screen. Or, use the phases to track specific properties of a\ngesture, as the gesture evolves."}),"\n",(0,o.jsx)(t.p,{children:"For touch point events, track how long the user rests on a specific interactive\nobject. An application can track multiple, simultaneous touch points' phases\nindividually, and handle each accordingly."}),"\n",(0,o.jsx)(t.p,{children:"For a gesture, interpret specific information about the transformation of the\ngesture as it occurs. Track the coordinates of the point of contact (or several)\nas they move across the screen."})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var i=n(96540);const o={},s=i.createContext(o);function r(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:t},e.children)}},47044:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/ig_touch_flow_popup-1b493466870f4bd2752ad414f25ebbfa.png"}}]);